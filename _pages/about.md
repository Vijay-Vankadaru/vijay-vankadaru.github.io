---
permalink: /
title: "ML Research Engineer & Aspiring PhD Candidate"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome! I'm Vijay Vankadaru, a machine learning researcher and engineer currently pursuing my Master's in Information and Data Science at UC Berkeley. With over four years of experience developing production ML systems in healthcare, I'm preparing for PhD studies to advance the methodological foundations of multimodal AI systems, particularly in audio and language understanding.

Research Focus
======

My research centers on developing interpretable and efficient machine learning models for multimodal content analysis, with particular emphasis on audio-NLP fusion and clinical applications. I work at the intersection of signal processing, natural language processing, and multimodal deep learning to create AI systems that can understand complex human communication patterns across multiple modalities.

**Current Research Areas:**
- **Multimodal Audio-Language Systems**: Developing neural architectures that combine acoustic features with linguistic content for comprehensive understanding
- **Clinical NLP and Audio Analysis**: Building interpretable reasoning systems for mental health assessment using voice biomarkers and conversational analysis
- **Efficient Multimodal Architectures**: Creating real-time processing systems that handle audio, text, and temporal information simultaneously
- **Interpretable Machine Learning**: Designing transparent AI systems suitable for healthcare deployment with step-by-step reasoning capabilities
- **Cross-Modal Knowledge Transfer**: Exploring how linguistic understanding can enhance audio analysis and vice versa

Professional Experience
======

I am a Founding Machine Learning Engineer/Researcher at DASION, a healthcare AI startup, where I have spent the past four years developing sophisticated machine learning systems for clinical applications. My work spans multiple modalities and includes:

**Multimodal Clinical AI Systems:**
- **Voice-Language Analysis**: Developed integrated systems analyzing both acoustic voice patterns and conversational content for depression detection, combining spectral analysis with NLP techniques for comprehensive assessment
- **Clinical Reasoning Frameworks**: Built interpretable AI systems that process audio biomarkers through structured reasoning steps, providing clinically actionable insights with transparent decision-making
- **Real-time Processing Pipelines**: Architected systems handling simultaneous speech recognition, sentiment analysis, and acoustic feature extraction with sub-second latency

**Production ML Infrastructure:**
- **Computer Vision Applications**: Built medical imaging analysis systems achieving 93% diagnostic accuracy across multiple clinical datasets, with integrated report generation using NLP techniques
- **Scalable Cloud Architecture**: Designed serverless systems processing multimodal clinical data (audio, text, images) with 99.9% uptime and HIPAA compliance
- **Cross-Modal Data Integration**: Developed pipelines combining patient voice recordings, clinical notes, and structured health data for comprehensive assessment

I also serve as CTO and Co-Founder of AGMNT, where I built multimodal recommendation systems analyzing user behavior, product descriptions, and interaction patterns to serve 10,000+ users across 15+ partner brands.

Current Research Projects
======

**Clinical-Grade Audio Reasoning for Depression Detection** *(In Progress)*

A comprehensive clinical validation study developing interpretable multimodal reasoning systems for mental health assessment. This work combines acoustic voice analysis with conversational NLP to create AI systems that provide transparent, step-by-step clinical reasoning. The system processes both voice biomarkers (fundamental frequency, pause patterns, voice quality) and linguistic features (sentiment, semantic content, discourse markers) through parallel reasoning pathways suitable for healthcare deployment.

**Ultra-Fast Reasoning Models for Multimodal Content Intelligence** *(In Progress)*

Research on efficient neural architectures that achieve real-time multimodal analysis through parallel reasoning frameworks and cross-modal knowledge distillation. This work explores how linguistic understanding can accelerate audio processing and how acoustic features can enhance NLP tasks, with applications in content moderation, engagement prediction, and real-time user experience optimization.

Both projects involve novel approaches to multimodal fusion, interpretable AI architectures, and efficient processing systems. Publications are planned for submission to top-tier ML and NLP venues in 2025, with code and datasets to be made available in my research portfolio.

Technical Expertise
======

**Machine Learning & AI:**
- Deep learning architectures (CNNs, RNNs, Transformers, attention mechanisms)
- Multimodal fusion techniques and cross-modal knowledge transfer
- Production ML systems (TensorFlow, PyTorch, distributed training)
- Real-time inference optimization and model compression

**Natural Language Processing:**
- Speech recognition and natural language understanding
- Sentiment analysis and conversational AI systems
- Clinical NLP for healthcare applications
- Large language model fine-tuning and deployment

**Audio & Signal Processing:**
- Acoustic feature extraction and voice biomarker analysis
- Real-time audio processing and streaming systems
- Multimodal audio-text alignment and synchronization
- Clinical voice analysis for healthcare applications

Academic Background & Collaborations
======

- **M.S. Information and Data Science**, UC Berkeley (Expected 2026)
- **B.S. Computer Science**, UC Riverside (2025)

**Research Collaborations:**
- **Professor Weiqing Gu** (Harvey Mudd College): NSF-funded research on voice-based depression detection, focusing on clinical validation and multimodal assessment approaches
- **Berkeley MIDS Faculty**: Exploring collaborations in computational social science, NLP applications, and efficient ML systems

I'm currently seeking additional research collaborations and preparing for PhD applications in Machine Learning, Natural Language Processing, and Computer Science for Fall 2026. My goal is to develop methodologically rigorous approaches to multimodal AI that advance both theoretical understanding and practical healthcare applications.

Research Philosophy
======

I believe the most impactful AI research combines technical innovation with real-world validation across multiple modalities. My approach emphasizes building systems that not only advance the state-of-the-art in individual domains (audio, NLP, vision) but also work reliably when these modalities are integrated in practice. This focus on multimodal understanding, clinical validation, and production deployment drives my work across healthcare applications, startup development, and academic research.

The future of AI lies in systems that can understand human communication as comprehensively as humans do - combining what we say, how we say it, and the context in which we communicate. My research aims to build these integrated understanding systems while maintaining the interpretability and reliability necessary for high-stakes applications like healthcare.

---

*For more details about my research projects, publications in progress, technical implementations, and multimodal AI work, please explore the navigation menu above.*